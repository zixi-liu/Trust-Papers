## Other Resources

### 迁移学习导论

#### 1. 背景与概念

迁移学习可以利用数据、任务或模型之间的相似性，将在旧领域学习过的模型和知识应用于新的领域。

迁移学习可以在训练数据和测试数据服从不同的数据概率分布时，更好地构建模型。

**少标注**
- 寻找一些与目标数据相近的有标注的数据，利用这些数据来构建模型。

**计算能力**
- 基于预训练模型进行微调

**泛化能力**
- Domain Adaptation

**分类**
- 按特征空间分类
  - 同构/异构
- 按目标域有无标签
- 按学习方法分类

**应用**
- CV
  - 同一类图片不同角度拍摄，都会造成特征分布发生改变。使用迁移学习构建跨领域的鲁棒分类器十分重要。

#### 3. 迁移学习基本问题

**何处迁移 Source Domain Selection**
- 数据集
- 样本层

#### 4. 方法与技术

迁移学习的核心是找到源域与目标域之间的相似性。度量目标有两个：1）度量两个领域的相似性；2）以度量为准则，增大两个领域之间的相似性。

**迁移学习方法**
- 样本权重迁移
- 统计特征变换迁移
  - 学习特征变换函数T，寻求一种显式或隐式的度量，使得在此距离度量下，源域和目标域的数据分布差异可以减小。
    - Maximum Mean Discrepancy(MMD)
    - 度量学习法
- 几何特征变换迁移
- 预训练
- 深度迁移学习


### 知乎 - 小王爱迁移

- [《小王爱迁移》系列之零：迁移学习领域著名学者和研究机构](https://zhuanlan.zhihu.com/p/30685086)
- [《小王爱迁移》系列之一：迁移成分分析(TCA)方法简介](https://zhuanlan.zhihu.com/p/26764147)
  - [迁移成分分析 TCA](https://zhuanlan.zhihu.com/p/60605059)
- [《小王爱迁移》系列之二：联合分布适配(JDA)方法简介](https://zhuanlan.zhihu.com/p/27336930)
- [《小王爱迁移》系列之三：深度神经网络的可迁移性](https://zhuanlan.zhihu.com/p/27450288)
  - AlexNet:前面几层都学习到的是通用的特征（general feature），随着网络的加深，后面的网络更偏重于学习特定的特征（specific feature）.
  - 如果应用于迁移学习，如何决定该迁移哪些层固定哪些层？
  - ![image](https://github.com/user-attachments/assets/99fecebc-36bc-47df-a787-2b9aa6270ce3)
    - 神经网络的前3层基本都是general feature，进行迁移的效果会比较好；
    - 深度迁移网络中加入fine-tune，效果会提升比较大，可能会比原网络效果还好；
    - 深度迁移网络要比随机初始化权重效果好；
    - Fine-tune可以比较好地克服数据之间的差异性；
    - Fine-tune: 如果目标数据集较小，模型的参数较多，那为了避免过拟合，可以将前面几层参数固定，只训练后几层；如果数据集较大，模型参数并不多，则可以全部进行训练。
    - [深度学习神经网络的特征可迁移性研究（NIPS）--论文笔记](https://zhuanlan.zhihu.com/p/27792786)
- [《小王爱迁移》系列之四：深度迁移网络](https://zhuanlan.zhihu.com/p/27657910)
  - 对于一个深度网络，随着网络层数的加深，网络越来越依赖于特定任务；而浅层相对来说只是学习一个大概的特征。如果要适配一个网络，重点是要适配高层——那些task-specific的层。
  - **DaNN(Domain Adaptive Neural Network)**
  - **DDC**
  - **DAN**
- [《小王爱迁移》系列之五：测地线流式核方法（GFK）](https://zhuanlan.zhihu.com/p/27782708)
- [《小王爱迁移》系列之六：从经验中学习迁移](https://zhuanlan.zhihu.com/p/28888554)
- [《小王爱迁移》系列之七：负迁移](https://zhuanlan.zhihu.com/p/30108427)
- [机器学习和统计学中常见的距离和相似度度量](https://zhuanlan.zhihu.com/p/27305237)
