## 综述

- [Mixture of Experts（MoE）学习笔记](https://zhuanlan.zhihu.com/p/675216281)
  - MoE通过weight function计算一个权重来将任务给到具体的模型来解决问题, 将一个大的问题空间拆分成小的子空间交由不同Expert解决。
  - [**Deep Mixture of Experts**](https://arxiv.org/pdf/1312.4314)
  - [**Sparsely-gated MoE layer**](https://arxiv.org/pdf/1701.06538)
    - <img width="601" alt="image" src="https://github.com/user-attachments/assets/9bc8eaa3-d514-4c88-b0c5-ec00bc0a111b" />
  - [**Noisy Top-K Gating**]
  - [**Balancing Expert Utilization**]
  - MoE与Transformer
    - ST-MoE（Stable and Transferable）

## [月球大叔EP07]

[[FIXME][EP07] 聊聊MoE + 闲谈学术品位](https://www.youtube.com/watch?v=mHUBwzlsWjg)
